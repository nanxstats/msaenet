[{"path":"https://nanx.me/msaenet/CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (https://contributor-covenant.org), version 1.0.0, available https://contributor-covenant.org/version/1/0/0/.","code":""},{"path":"https://nanx.me/msaenet/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to msaenet","title":"Contributing to msaenet","text":"üëçüéâ First , thanks taking time contribute! üéâüëç contribute project : Filing bug report feature request issue. Suggesting change via pull request.","code":""},{"path":"https://nanx.me/msaenet/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"","what":"Issues","title":"Contributing to msaenet","text":"file issue possible bug, please try include: Relevant package versions Necessary code data reproduce issue","code":""},{"path":"https://nanx.me/msaenet/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull Requests","title":"Contributing to msaenet","text":"suggest change via pull requests, please: Fork repository GitHub account. Clone forked repository local machine, make changes. Commit push changes GitHub. Create pull request.","code":""},{"path":"https://nanx.me/msaenet/articles/msaenet.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"A Quick Introduction to msaenet","text":"msaenet package implemented multi-step adaptive elastic-net method introduced Xiao Xu (2015) feature selection high-dimensional regressions.","code":""},{"path":"https://nanx.me/msaenet/articles/msaenet.html","id":"walkthrough","dir":"Articles","previous_headings":"","what":"Walkthrough","title":"A Quick Introduction to msaenet","text":"Let‚Äôs load package: First, generate simulated data setting often used testing high-dimensional linear models, function msaenet.sim.gaussian(): parameter rho controls degree correlation among variables. coef sets coefficients ‚Äútrue‚Äù variables, case, first 10 variables coefficient 1 490 variables coefficient 0. snr represents designated signal--noise ratio (SNR) simulated data. parameter p.train decides proportion training set (relative total number observations n). generate simulation data types generalized linear models supported msaenet, simply use msaenet.sim.binomial() (logistic regression), msaenet.sim.cox() (Cox regression), msaenet.sim.poisson() (Poisson regression). returned object dat contains training test set. use training set modeling (parameter tuning model fitting), evaluate model‚Äôs performance test set independently. parameter alphas sets alpha tuning grid elastic-net adaptive estimation steps. nsteps indicates many adaptive estimation steps used. default, internal parameter tuning done k-fold cross-validation, parameters produce minimum prediction errors selected. also set parallel = TRUE run calling function make parameter tuning run parallel. probably save time alphas grid denser data size larger. select optimal model estimation step different criterion, use argument tune. Options include \"cv\" (k-fold cross-validation, default), \"aic\" (AIC), \"bic\" (BIC), \"ebic\" (Extened BIC). Similarly, use tune.nsteps specify criterion selecting optimal estimation step (optimal model steps), options include \"max\" (select final-step, default), \"aic\", \"bic\", \"ebic\". Let‚Äôs inspect fitted model, looking best step selected variables (variables non-zero coefficients), number false positive selections/true positive selections: Next, make predictions test set using fitted model, compute evaluation metrics, RMSE MAE: coefficient plot shows coefficient changes variables across every adaptive estimation step:  y-axis plot represents relative effect size estimations (standardized [0, 1]) variables. Plot change information criterion (EBIC ) used select optimal step:  Create Cleveland dot plot model coefficients optimal step:  plot absolute values coefficients instead raw coefficients, use abs = TRUE. vanilla adaptive elastic-net (Zou Zhang 2009) implemented function aenet(). multi-step adaptive estimation based MCP-net SCAD-net, see ?amnet, ?asnet, ?msamnet, ?msasnet details. analyses apply models fitted functions well.","code":"library(\"msaenet\") dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.5,   coef = rep(1, 10), snr = 5, p.train = 0.7,   seed = 1001 ) msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.1, 0.9, 0.1),   nsteps = 10L, tune.nsteps = \"ebic\",   seed = 1005 ) library(\"doParallel\") registerDoParallel(detectCores()) msaenet.fit$best.step #> [1] 6 msaenet.nzv(msaenet.fit) #>  [1]   2   3   4   6   7   9  10  35 363 379 msaenet.nzv.all(msaenet.fit) #> [[1]] #>  [1]   1   2   3   4   5   6   7   8   9  10  22  33  35  47 124 183 191 225 266 #> [20] 269 312 325 334 363 379 388 396 449 #>  #> [[2]] #>  [1]   1   2   3   4   5   6   7   8   9  10  35 269 363 379 #>  #> [[3]] #>  [1]   2   3   4   5   6   7   8   9  10  35 269 363 379 #>  #> [[4]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[5]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[6]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[7]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[8]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[9]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[10]] #>  [1]   2   3   4   6   7   9  10  35 363 379 #>  #> [[11]] #>  [1]   2   3   4   6   7   9  10  35 363 379 msaenet.fp(msaenet.fit, 1:10) #> [1] 3 msaenet.tp(msaenet.fit, 1:10) #> [1] 7 msaenet.pred <- predict(msaenet.fit, dat$x.te) msaenet.rmse(dat$y.te, msaenet.pred) #> [1] 2.638096 msaenet.mae(dat$y.te, msaenet.pred) #> [1] 2.172675 plot(msaenet.fit, label = TRUE) plot(msaenet.fit, type = \"criterion\") plot(msaenet.fit, type = \"dotplot\", label = TRUE, label.cex = 1)"},{"path":"https://nanx.me/msaenet/articles/msaenet.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"A Quick Introduction to msaenet","text":"used msaenet research, please feel free cite paper (Xiao Xu 2015) publications. questions bug report, please email create issue GitHub.","code":""},{"path":[]},{"path":"https://nanx.me/msaenet/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Nan Xiao. Author, maintainer. Qing-Song Xu. Author.","code":""},{"path":"https://nanx.me/msaenet/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Nan Xiao Qing-Song Xu. (2015). Multi-step adaptive elastic-net: reducing false positives high-dimensional variable selection. Journal Statistical Computation Simulation 85(18), 3755-3765.","code":"@Article{,   title = {Multi-step adaptive elastic-net: reducing false positives in high-dimensional variable selection},   journal = {Journal of Statistical Computation and Simulation},   year = {2015},   author = {Nan Xiao and Qing-Song Xu},   volume = {85},   number = {18},   pages = {3755--3765},   doi = {10.1080/00949655.2015.1016944}, }"},{"path":"https://nanx.me/msaenet/index.html","id":"msaenet-","dir":"","previous_headings":"","what":"Multi-Step Adaptive Estimation Methods for Sparse Regressions","title":"Multi-Step Adaptive Estimation Methods for Sparse Regressions","text":"msaenet implements multi-step adaptive elastic-net (MSAENet) algorithm feature selection high-dimensional regressions proposed Xiao Xu (2015) <DOI:10.1080/00949655.2015.1016944> (PDF). Nonconvex multi-step adaptive estimations based MCP-net SCAD-net also supported. Check vignette(\"msaenet\") quick-start.","code":""},{"path":"https://nanx.me/msaenet/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Multi-Step Adaptive Estimation Methods for Sparse Regressions","text":"can install msaenet CRAN: try development version GitHub:","code":"install.packages(\"msaenet\") remotes::install_github(\"nanxstats/msaenet\")"},{"path":"https://nanx.me/msaenet/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Multi-Step Adaptive Estimation Methods for Sparse Regressions","text":"Nan Xiao Qing-Song Xu. (2015). Multi-step adaptive elastic-net: reducing false positives high-dimensional variable selection. Journal Statistical Computation Simulation 85(18), 3755‚Äì3765.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://nanx.me/msaenet/index.html","id":"contribute","dir":"","previous_headings":"","what":"Contribute","title":"Multi-Step Adaptive Estimation Methods for Sparse Regressions","text":"contribute project, please take look Contributing Guidelines first. Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"https://nanx.me/msaenet/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Multi-Step Adaptive Estimation Methods for Sparse Regressions","text":"msaenet free open source software, licensed GPL-3.","code":""},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive Elastic-Net ‚Äî aenet","title":"Adaptive Elastic-Net ‚Äî aenet","text":"Adaptive Elastic-Net","code":""},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive Elastic-Net ‚Äî aenet","text":"","code":"aenet(   x,   y,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"cox\"),   init = c(\"enet\", \"ridge\"),   alphas = seq(0.05, 0.95, 0.05),   tune = c(\"cv\", \"ebic\", \"bic\", \"aic\"),   nfolds = 5L,   rule = c(\"lambda.min\", \"lambda.1se\"),   ebic.gamma = 1,   scale = 1,   lower.limits = -Inf,   upper.limits = Inf,   penalty.factor.init = rep(1, ncol(x)),   seed = 1001,   parallel = FALSE,   verbose = FALSE )"},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive Elastic-Net ‚Äî aenet","text":"x Data matrix. y Response vector family \"gaussian\", \"binomial\", \"poisson\". family \"cox\", response matrix created Surv. family Model family, can \"gaussian\", \"binomial\", \"poisson\", \"cox\". init Type penalty used initial estimation step. Can \"enet\" \"ridge\". alphas Vector candidate alphas use cv.glmnet. tune Parameter tuning method estimation step. Possible options \"cv\", \"ebic\", \"bic\", \"aic\". Default \"cv\". nfolds Fold numbers cross-validation tune = \"cv\". rule Lambda selection criterion tune = \"cv\", can \"lambda.min\" \"lambda.1se\". See cv.glmnet details. ebic.gamma Parameter Extended BIC penalizing size model space tune = \"ebic\", default 1. details, see Chen Chen (2008). scale Scaling factor adaptive weights: weights = coefficients^(-scale). lower.limits Lower limits coefficients. Default -Inf. details, see glmnet. upper.limits Upper limits coefficients. Default Inf. details, see glmnet. penalty.factor.init multiplicative factor penalty applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. make certain variables likely selected, assign smaller value. Default rep(1, ncol(x)). seed Random seed cross-validation fold division. parallel Logical. Enable parallel parameter tuning , default FALSE. enable parallel tuning, load doParallel package run registerDoParallel() number CPU cores calling function. verbose print estimation progress?","code":""},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive Elastic-Net ‚Äî aenet","text":"List model coefficients, glmnet model object, optimal parameter set.","code":""},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adaptive Elastic-Net ‚Äî aenet","text":"Zou, Hui, Hao Helen Zhang. (2009). adaptive elastic-net diverging number parameters. Annals Statistics 37(4), 1733--1751.","code":""},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Adaptive Elastic-Net ‚Äî aenet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/aenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive Elastic-Net ‚Äî aenet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  aenet.fit <- aenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2), seed = 1002 )  print(aenet.fit) #> Call: aenet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.2, 0.8, 0.2),  #>     seed = 1002)  #>   Df      %Dev       Lambda #> 1 12 0.7962207 9.644864e+14 msaenet.nzv(aenet.fit) #>  [1]   2   3   4   5  33  35  49 114 269 363 379 441 msaenet.fp(aenet.fit, 1:5) #> [1] 8 msaenet.tp(aenet.fit, 1:5) #> [1] 4 aenet.pred <- predict(aenet.fit, dat$x.te) msaenet.rmse(dat$y.te, aenet.pred) #> [1] 2.640474 plot(aenet.fit)"},{"path":"https://nanx.me/msaenet/reference/amnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive MCP-Net ‚Äî amnet","title":"Adaptive MCP-Net ‚Äî amnet","text":"Adaptive MCP-Net","code":""},{"path":"https://nanx.me/msaenet/reference/amnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive MCP-Net ‚Äî amnet","text":"","code":"amnet(   x,   y,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"cox\"),   init = c(\"mnet\", \"ridge\"),   gammas = 3,   alphas = seq(0.05, 0.95, 0.05),   tune = c(\"cv\", \"ebic\", \"bic\", \"aic\"),   nfolds = 5L,   ebic.gamma = 1,   scale = 1,   eps = 1e-04,   max.iter = 10000L,   penalty.factor.init = rep(1, ncol(x)),   seed = 1001,   parallel = FALSE,   verbose = FALSE )"},{"path":"https://nanx.me/msaenet/reference/amnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive MCP-Net ‚Äî amnet","text":"x Data matrix. y Response vector family \"gaussian\", \"binomial\", \"poisson\". family \"cox\", response matrix created Surv. family Model family, can \"gaussian\", \"binomial\", \"poisson\", \"cox\". init Type penalty used initial estimation step. Can \"mnet\" \"ridge\". gammas Vector candidate gammas (concavity parameter) use MCP-Net. Default 3. alphas Vector candidate alphas use MCP-Net. tune Parameter tuning method estimation step. Possible options \"cv\", \"ebic\", \"bic\", \"aic\". Default \"cv\". nfolds Fold numbers cross-validation tune = \"cv\". ebic.gamma Parameter Extended BIC penalizing size model space tune = \"ebic\", default 1. details, see Chen Chen (2008). scale Scaling factor adaptive weights: weights = coefficients^(-scale). eps Convergence threshhold use MCP-net. max.iter Maximum number iterations use MCP-net. penalty.factor.init multiplicative factor penalty applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. make certain variables likely selected, assign smaller value. Default rep(1, ncol(x)). seed Random seed cross-validation fold division. parallel Logical. Enable parallel parameter tuning , default FALSE. enable parallel tuning, load doParallel package run registerDoParallel() number CPU cores calling function. verbose print estimation progress?","code":""},{"path":"https://nanx.me/msaenet/reference/amnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive MCP-Net ‚Äî amnet","text":"List model coefficients, ncvreg model object, optimal parameter set.","code":""},{"path":"https://nanx.me/msaenet/reference/amnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Adaptive MCP-Net ‚Äî amnet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/amnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive MCP-Net ‚Äî amnet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  amnet.fit <- amnet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2), seed = 1002 ) #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values.  print(amnet.fit) #> Call: amnet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.2, 0.8, 0.2),  #>     seed = 1002)  #>   Df    Lambda Gamma Alpha #> 1  5 0.2826426     3   0.8 msaenet.nzv(amnet.fit) #> [1]   2   4   5  35 269 msaenet.fp(amnet.fit, 1:5) #> [1] 2 msaenet.tp(amnet.fit, 1:5) #> [1] 3 amnet.pred <- predict(amnet.fit, dat$x.te) msaenet.rmse(dat$y.te, amnet.pred) #> [1] 2.67195 plot(amnet.fit)"},{"path":"https://nanx.me/msaenet/reference/asnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive SCAD-Net ‚Äî asnet","title":"Adaptive SCAD-Net ‚Äî asnet","text":"Adaptive SCAD-Net","code":""},{"path":"https://nanx.me/msaenet/reference/asnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive SCAD-Net ‚Äî asnet","text":"","code":"asnet(   x,   y,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"cox\"),   init = c(\"snet\", \"ridge\"),   gammas = 3.7,   alphas = seq(0.05, 0.95, 0.05),   tune = c(\"cv\", \"ebic\", \"bic\", \"aic\"),   nfolds = 5L,   ebic.gamma = 1,   scale = 1,   eps = 1e-04,   max.iter = 10000L,   penalty.factor.init = rep(1, ncol(x)),   seed = 1001,   parallel = FALSE,   verbose = FALSE )"},{"path":"https://nanx.me/msaenet/reference/asnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive SCAD-Net ‚Äî asnet","text":"x Data matrix. y Response vector family \"gaussian\", \"binomial\", \"poisson\". family \"cox\", response matrix created Surv. family Model family, can \"gaussian\", \"binomial\", \"poisson\", \"cox\". init Type penalty used initial estimation step. Can \"snet\" \"ridge\". gammas Vector candidate gammas (concavity parameter) use SCAD-Net. Default 3.7. alphas Vector candidate alphas use SCAD-Net. tune Parameter tuning method estimation step. Possible options \"cv\", \"ebic\", \"bic\", \"aic\". Default \"cv\". nfolds Fold numbers cross-validation tune = \"cv\". ebic.gamma Parameter Extended BIC penalizing size model space tune = \"ebic\", default 1. details, see Chen Chen (2008). scale Scaling factor adaptive weights: weights = coefficients^(-scale). eps Convergence threshhold use SCAD-net. max.iter Maximum number iterations use SCAD-net. penalty.factor.init multiplicative factor penalty applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. make certain variables likely selected, assign smaller value. Default rep(1, ncol(x)). seed Random seed cross-validation fold division. parallel Logical. Enable parallel parameter tuning , default FALSE. enable parallel tuning, load doParallel package run registerDoParallel() number CPU cores calling function. verbose print estimation progress?","code":""},{"path":"https://nanx.me/msaenet/reference/asnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adaptive SCAD-Net ‚Äî asnet","text":"List model coefficients, ncvreg model object, optimal parameter set.","code":""},{"path":"https://nanx.me/msaenet/reference/asnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Adaptive SCAD-Net ‚Äî asnet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/asnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adaptive SCAD-Net ‚Äî asnet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  asnet.fit <- asnet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2), seed = 1002 ) #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values.  print(asnet.fit) #> Call: asnet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.2, 0.8, 0.2),  #>     seed = 1002)  #>   Df    Lambda Gamma Alpha #> 1  4 0.3104765   3.7   0.8 msaenet.nzv(asnet.fit) #> [1]  2  4  5 35 msaenet.fp(asnet.fit, 1:5) #> [1] 1 msaenet.tp(asnet.fit, 1:5) #> [1] 3 asnet.pred <- predict(asnet.fit, dat$x.te) msaenet.rmse(dat$y.te, asnet.pred) #> [1] 2.693864 plot(asnet.fit)"},{"path":"https://nanx.me/msaenet/reference/coef.msaenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Model Coefficients ‚Äî coef.msaenet","title":"Extract Model Coefficients ‚Äî coef.msaenet","text":"Extract model coefficients final model msaenet model objects.","code":""},{"path":"https://nanx.me/msaenet/reference/coef.msaenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Model Coefficients ‚Äî coef.msaenet","text":"","code":"# S3 method for msaenet coef(object, ...)"},{"path":"https://nanx.me/msaenet/reference/coef.msaenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Model Coefficients ‚Äî coef.msaenet","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet. ... Additional parameters coef (used).","code":""},{"path":"https://nanx.me/msaenet/reference/coef.msaenet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Model Coefficients ‚Äî coef.msaenet","text":"numerical vector model coefficients.","code":""},{"path":"https://nanx.me/msaenet/reference/coef.msaenet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract Model Coefficients ‚Äî coef.msaenet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/coef.msaenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Model Coefficients ‚Äî coef.msaenet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  coef(msaenet.fit) #>   [1]  0.0000000  2.1657439  0.0000000  1.7619827  0.8366843  0.0000000 #>   [7]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [13]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [19]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [25]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [31]  0.0000000  0.0000000  0.0000000  0.0000000  0.8954118  0.0000000 #>  [37]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [43]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [49]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [55]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [61]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [67]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [73]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [79]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [85]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [91]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #>  [97]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [103]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [109]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 -0.3592967 #> [115]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [121]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [127]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [133]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [139]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [145]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [151]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [157]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [163]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [169]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [175]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [181]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [187]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [193]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [199]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [205]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [211]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [217]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [223]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [229]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [235]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [241]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [247]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [253]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [259]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [265]  0.0000000  0.0000000  0.0000000  0.0000000  0.7274414  0.0000000 #> [271]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [277]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [283]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [289]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [295]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [301]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [307]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [313]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [319]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [325]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [331]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [337]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [343]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [349]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [355]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [361]  0.0000000  0.0000000 -0.4787929  0.0000000  0.0000000  0.0000000 #> [367]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [373]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [379] -0.2787463  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [385]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [391]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [397]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [403]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [409]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [415]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [421]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [427]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [433]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [439]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [445]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [451]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [457]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [463]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [469]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [475]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [481]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [487]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [493]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 #> [499]  0.0000000  0.0000000"},{"path":"https://nanx.me/msaenet/reference/msaenet-package.html","id":null,"dir":"Reference","previous_headings":"","what":"msaenet: Multi-Step Adaptive Estimation Methods for Sparse Regressions ‚Äî msaenet-package","title":"msaenet: Multi-Step Adaptive Estimation Methods for Sparse Regressions ‚Äî msaenet-package","text":"Multi-step adaptive elastic-net (MSAENet) algorithm feature selection high-dimensional regressions proposed Xiao Xu (2015) doi:10.1080/00949655.2015.1016944 , support multi-step adaptive MCP-net (MSAMNet) multi-step adaptive SCAD-net (MSASNet) methods.","code":""},{"path":[]},{"path":"https://nanx.me/msaenet/reference/msaenet-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"msaenet: Multi-Step Adaptive Estimation Methods for Sparse Regressions ‚Äî msaenet-package","text":"Maintainer: Nan Xiao @nanx.(ORCID) Authors: Qing-Song Xu qsxu@csu.edu.cn","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fn.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Number of False Negative Selections ‚Äî msaenet.fn","title":"Get the Number of False Negative Selections ‚Äî msaenet.fn","text":"Get number false negative selections msaenet model objects, given indices true variables (known).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Number of False Negative Selections ‚Äî msaenet.fn","text":"","code":"msaenet.fn(object, true.idx)"},{"path":"https://nanx.me/msaenet/reference/msaenet.fn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Number of False Negative Selections ‚Äî msaenet.fn","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet. true.idx Vector. Indices true variables.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Number of False Negative Selections ‚Äî msaenet.fn","text":"Number false negative variables model.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the Number of False Negative Selections ‚Äî msaenet.fn","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the Number of False Negative Selections ‚Äî msaenet.fn","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  msaenet.fn(msaenet.fit, 1:5) #> [1] 2"},{"path":"https://nanx.me/msaenet/reference/msaenet.fp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Number of False Positive Selections ‚Äî msaenet.fp","title":"Get the Number of False Positive Selections ‚Äî msaenet.fp","text":"Get number false positive selections msaenet model objects, given indices true variables (known).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Number of False Positive Selections ‚Äî msaenet.fp","text":"","code":"msaenet.fp(object, true.idx)"},{"path":"https://nanx.me/msaenet/reference/msaenet.fp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Number of False Positive Selections ‚Äî msaenet.fp","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet. true.idx Vector. Indices true variables.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Number of False Positive Selections ‚Äî msaenet.fp","text":"Number false positive variables model.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the Number of False Positive Selections ‚Äî msaenet.fp","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.fp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the Number of False Positive Selections ‚Äî msaenet.fp","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  msaenet.fp(msaenet.fit, 1:5) #> [1] 5"},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"Multi-Step Adaptive Elastic-Net","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"","code":"msaenet(   x,   y,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"cox\"),   init = c(\"enet\", \"ridge\"),   alphas = seq(0.05, 0.95, 0.05),   tune = c(\"cv\", \"ebic\", \"bic\", \"aic\"),   nfolds = 5L,   rule = c(\"lambda.min\", \"lambda.1se\"),   ebic.gamma = 1,   nsteps = 2L,   tune.nsteps = c(\"max\", \"ebic\", \"bic\", \"aic\"),   ebic.gamma.nsteps = 1,   scale = 1,   lower.limits = -Inf,   upper.limits = Inf,   penalty.factor.init = rep(1, ncol(x)),   seed = 1001,   parallel = FALSE,   verbose = FALSE )"},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"x Data matrix. y Response vector family \"gaussian\", \"binomial\", \"poisson\". family \"cox\", response matrix created Surv. family Model family, can \"gaussian\", \"binomial\", \"poisson\", \"cox\". init Type penalty used initial estimation step. Can \"enet\" \"ridge\". See glmnet details. alphas Vector candidate alphas use cv.glmnet. tune Parameter tuning method estimation step. Possible options \"cv\", \"ebic\", \"bic\", \"aic\". Default \"cv\". nfolds Fold numbers cross-validation tune = \"cv\". rule Lambda selection criterion tune = \"cv\", can \"lambda.min\" \"lambda.1se\". See cv.glmnet details. ebic.gamma Parameter Extended BIC penalizing size model space tune = \"ebic\", default 1. details, see Chen Chen (2008). nsteps Maximum number adaptive estimation steps. least 2, assuming adaptive elastic-net one adaptive estimation step. tune.nsteps Optimal step number selection method (aggregate optimal model step compare). Options include \"max\" (select final-step model directly), compare models using \"ebic\", \"bic\", \"aic\". Default \"max\". ebic.gamma.nsteps Parameter Extended BIC penalizing size model space tune.nsteps = \"ebic\", default 1. scale Scaling factor adaptive weights: weights = coefficients^(-scale). lower.limits Lower limits coefficients. Default -Inf. details, see glmnet. upper.limits Upper limits coefficients. Default Inf. details, see glmnet. penalty.factor.init multiplicative factor penalty applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. make certain variables likely selected, assign smaller value. Default rep(1, ncol(x)). seed Random seed cross-validation fold division. parallel Logical. Enable parallel parameter tuning , default FALSE. enable parallel tuning, load doParallel package run registerDoParallel() number CPU cores calling function. verbose print estimation progress?","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"List model coefficients, glmnet model object, optimal parameter set.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"Nan Xiao Qing-Song Xu. (2015). Multi-step adaptive elastic-net: reducing false positives high-dimensional variable selection. Journal Statistical Computation Simulation 85(18), 3755--3765.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-Step Adaptive Elastic-Net ‚Äî msaenet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  print(msaenet.fit) #> Call: msaenet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.2, 0.8, 0.2),  #>     nsteps = 3L, seed = 1003)  #>   Df     %Dev       Lambda #> 1  8 0.797121 1.219202e+15 msaenet.nzv(msaenet.fit) #> [1]   2   4   5  35 114 269 363 379 msaenet.fp(msaenet.fit, 1:5) #> [1] 5 msaenet.tp(msaenet.fit, 1:5) #> [1] 3 msaenet.pred <- predict(msaenet.fit, dat$x.te) msaenet.rmse(dat$y.te, msaenet.pred) #> [1] 2.839212 plot(msaenet.fit)"},{"path":"https://nanx.me/msaenet/reference/msaenet.mae.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Absolute Error (MAE) ‚Äî msaenet.mae","title":"Mean Absolute Error (MAE) ‚Äî msaenet.mae","text":"Compute mean absolute error (MAE).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mae.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Absolute Error (MAE) ‚Äî msaenet.mae","text":"","code":"msaenet.mae(yreal, ypred)"},{"path":"https://nanx.me/msaenet/reference/msaenet.mae.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Absolute Error (MAE) ‚Äî msaenet.mae","text":"yreal Vector. True response. ypred Vector. Predicted response.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mae.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean Absolute Error (MAE) ‚Äî msaenet.mae","text":"MAE","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mae.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Mean Absolute Error (MAE) ‚Äî msaenet.mae","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mse.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Squared Error (MSE) ‚Äî msaenet.mse","title":"Mean Squared Error (MSE) ‚Äî msaenet.mse","text":"Compute mean squared error (MSE).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Squared Error (MSE) ‚Äî msaenet.mse","text":"","code":"msaenet.mse(yreal, ypred)"},{"path":"https://nanx.me/msaenet/reference/msaenet.mse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Squared Error (MSE) ‚Äî msaenet.mse","text":"yreal Vector. True response. ypred Vector. Predicted response.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean Squared Error (MSE) ‚Äî msaenet.mse","text":"MSE","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.mse.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Mean Squared Error (MSE) ‚Äî msaenet.mse","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.all.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","title":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","text":"Get indices non-zero variables steps msaenet model objects.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","text":"","code":"msaenet.nzv.all(object)"},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","text":"List containing indices vectors non-zero variables steps.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.all.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.all.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Indices of Non-Zero Variables in All Steps ‚Äî msaenet.nzv.all","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  msaenet.nzv.all(msaenet.fit) #> [[1]] #>  [1]   1   2   3   4   5   6  33  35  49  73 114 145 183 235 269 334 363 379 441 #> [20] 449 #>  #> [[2]] #>  [1]   2   3   4   5  35  49 114 269 363 379 #>  #> [[3]] #> [1]   2   3   4   5  35 114 269 363 379 #>  #> [[4]] #> [1]   2   4   5  35 114 269 363 379 #>"},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","title":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","text":"Get indices non-zero variables msaenet model objects.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","text":"","code":"msaenet.nzv(object)"},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","text":"Indices vector non-zero variables model.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.nzv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Indices of Non-Zero Variables ‚Äî msaenet.nzv","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  msaenet.nzv(msaenet.fit) #> [1]   2   4   5  35 114 269 363 379  # coefficients of non-zero variables coef(msaenet.fit)[msaenet.nzv(msaenet.fit)] #> [1]  2.1657439  1.7619827  0.8366843  0.8954118 -0.3592967  0.7274414 -0.4787929 #> [8] -0.2787463"},{"path":"https://nanx.me/msaenet/reference/msaenet.rmse.html","id":null,"dir":"Reference","previous_headings":"","what":"Root Mean Squared Error (RMSE) ‚Äî msaenet.rmse","title":"Root Mean Squared Error (RMSE) ‚Äî msaenet.rmse","text":"Compute root mean squared error (RMSE).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Root Mean Squared Error (RMSE) ‚Äî msaenet.rmse","text":"","code":"msaenet.rmse(yreal, ypred)"},{"path":"https://nanx.me/msaenet/reference/msaenet.rmse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Root Mean Squared Error (RMSE) ‚Äî msaenet.rmse","text":"yreal Vector. True response. ypred Vector. Predicted response.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Root Mean Squared Error (RMSE) ‚Äî msaenet.rmse","text":"RMSE","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmse.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Root Mean Squared Error (RMSE) ‚Äî msaenet.rmse","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmsle.html","id":null,"dir":"Reference","previous_headings":"","what":"Root Mean Squared Logarithmic Error (RMSLE) ‚Äî msaenet.rmsle","title":"Root Mean Squared Logarithmic Error (RMSLE) ‚Äî msaenet.rmsle","text":"Compute root mean squared logarithmic error (RMSLE).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmsle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Root Mean Squared Logarithmic Error (RMSLE) ‚Äî msaenet.rmsle","text":"","code":"msaenet.rmsle(yreal, ypred)"},{"path":"https://nanx.me/msaenet/reference/msaenet.rmsle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Root Mean Squared Logarithmic Error (RMSLE) ‚Äî msaenet.rmsle","text":"yreal Vector. True response. ypred Vector. Predicted response.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmsle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Root Mean Squared Logarithmic Error (RMSLE) ‚Äî msaenet.rmsle","text":"RMSLE","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.rmsle.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Root Mean Squared Logarithmic Error (RMSLE) ‚Äî msaenet.rmsle","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.binomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","text":"Generate simulation data benchmarking sparse logistic regression models.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.binomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","text":"","code":"msaenet.sim.binomial(   n = 300,   p = 500,   rho = 0.5,   coef = rep(0.2, 50),   snr = 1,   p.train = 0.7,   seed = 1001 )"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.binomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","text":"n Number observations. p Number variables. rho Correlation base generating correlated variables. coef Vector non-zero coefficients. snr Signal--noise ratio (SNR). p.train Percentage training set. seed Random seed reproducibility.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.binomial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","text":"List x.tr, x.te, y.tr, y.te.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.binomial.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.binomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Binomial Response) ‚Äî msaenet.sim.binomial","text":"","code":"dat <- msaenet.sim.binomial(   n = 300, p = 500, rho = 0.6,   coef = rep(1, 10), snr = 3, p.train = 0.7,   seed = 1001 )  dim(dat$x.tr) #> [1] 210 500 dim(dat$x.te) #> [1]  90 500 table(dat$y.tr) #>  #>   0   1  #> 101 109  table(dat$y.te) #>  #>  0  1  #> 48 42"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"Generate simulation data benchmarking sparse Cox regression models.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"","code":"msaenet.sim.cox(   n = 300,   p = 500,   rho = 0.5,   coef = rep(0.2, 50),   snr = 1,   p.train = 0.7,   seed = 1001 )"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"n Number observations. p Number variables. rho Correlation base generating correlated variables. coef Vector non-zero coefficients. snr Signal--noise ratio (SNR). p.train Percentage training set. seed Random seed reproducibility.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"List x.tr, x.te, y.tr, y.te.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"Simon, N., Friedman, J., Hastie, T., & Tibshirani, R. (2011). Regularization Paths Cox's Proportional Hazards Model via Coordinate Descent. Journal Statistical Software, 39(5), 1--13.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.cox.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model) ‚Äî msaenet.sim.cox","text":"","code":"dat <- msaenet.sim.cox(   n = 300, p = 500, rho = 0.6,   coef = rep(1, 10), snr = 3, p.train = 0.7,   seed = 1001 )  dim(dat$x.tr) #> [1] 210 500 dim(dat$x.te) #> [1]  90 500 dim(dat$y.tr) #> [1] 210   2 dim(dat$y.te) #> [1] 90  2"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"Generate simulation data (Gaussian case) following settings Xiao Xu (2015).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"","code":"msaenet.sim.gaussian(   n = 300,   p = 500,   rho = 0.5,   coef = rep(0.2, 50),   snr = 1,   p.train = 0.7,   seed = 1001 )"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"n Number observations. p Number variables. rho Correlation base generating correlated variables. coef Vector non-zero coefficients. snr Signal--noise ratio (SNR). SNR defined $$ \\frac{Var(E(y | X))}{Var(Y - E(y | X))} = \\frac{Var(f(X))}{Var(\\varepsilon)} = \\frac{Var(X^T \\beta)}{Var(\\varepsilon)} = \\frac{Var(\\beta^T \\Sigma \\beta)}{\\sigma^2}. $$ p.train Percentage training set. seed Random seed reproducibility.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"List x.tr, x.te, y.tr, y.te.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"Nan Xiao Qing-Song Xu. (2015). Multi-step adaptive elastic-net: reducing false positives high-dimensional variable selection. Journal Statistical Computation Simulation 85(18), 3755--3765.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.gaussian.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Gaussian Response) ‚Äî msaenet.sim.gaussian","text":"","code":"dat <- msaenet.sim.gaussian(   n = 300, p = 500, rho = 0.6,   coef = rep(1, 10), snr = 3, p.train = 0.7,   seed = 1001 )  dim(dat$x.tr) #> [1] 210 500 dim(dat$x.te) #> [1]  90 500"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.poisson.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","text":"Generate simulation data benchmarking sparse Poisson regression models.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.poisson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","text":"","code":"msaenet.sim.poisson(   n = 300,   p = 500,   rho = 0.5,   coef = rep(0.2, 50),   snr = 1,   p.train = 0.7,   seed = 1001 )"},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.poisson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","text":"n Number observations. p Number variables. rho Correlation base generating correlated variables. coef Vector non-zero coefficients. snr Signal--noise ratio (SNR). p.train Percentage training set. seed Random seed reproducibility.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.poisson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","text":"List x.tr, x.te, y.tr, y.te.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.poisson.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.sim.poisson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Simulation Data for Benchmarking Sparse Regressions\n(Poisson Response) ‚Äî msaenet.sim.poisson","text":"","code":"dat <- msaenet.sim.poisson(   n = 300, p = 500, rho = 0.6,   coef = rep(1, 10), snr = 3, p.train = 0.7,   seed = 1001 )  dim(dat$x.tr) #> [1] 210 500 dim(dat$x.te) #> [1]  90 500"},{"path":"https://nanx.me/msaenet/reference/msaenet.tp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Number of True Positive Selections ‚Äî msaenet.tp","title":"Get the Number of True Positive Selections ‚Äî msaenet.tp","text":"Get number true positive selections msaenet model objects, given indices true variables (known).","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Number of True Positive Selections ‚Äî msaenet.tp","text":"","code":"msaenet.tp(object, true.idx)"},{"path":"https://nanx.me/msaenet/reference/msaenet.tp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Number of True Positive Selections ‚Äî msaenet.tp","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet. true.idx Vector. Indices true variables.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Number of True Positive Selections ‚Äî msaenet.tp","text":"Number true positive variables model.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the Number of True Positive Selections ‚Äî msaenet.tp","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the Number of True Positive Selections ‚Äî msaenet.tp","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  msaenet.tp(msaenet.fit, 1:5) #> [1] 3"},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatic (parallel) parameter tuning for glmnet models ‚Äî msaenet.tune.glmnet","title":"Automatic (parallel) parameter tuning for glmnet models ‚Äî msaenet.tune.glmnet","text":"Automatic (parallel) parameter tuning glmnet models","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatic (parallel) parameter tuning for glmnet models ‚Äî msaenet.tune.glmnet","text":"","code":"msaenet.tune.glmnet(   x,   y,   family,   alphas,   tune,   nfolds,   rule,   ebic.gamma,   lower.limits,   upper.limits,   seed,   parallel,   ... )"},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatic (parallel) parameter tuning for glmnet models ‚Äî msaenet.tune.glmnet","text":"Optimal model object, parameter set, criterion value","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.glmnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Automatic (parallel) parameter tuning for glmnet models ‚Äî msaenet.tune.glmnet","text":"Chen, Jiahua, Zehua Chen. (2008). Extended Bayesian information criteria model selection large model spaces. Biometrika 95(3), 759--771.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.glmnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Automatic (parallel) parameter tuning for glmnet models ‚Äî msaenet.tune.glmnet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.ncvreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatic (parallel) parameter tuning for ncvreg models ‚Äî msaenet.tune.ncvreg","title":"Automatic (parallel) parameter tuning for ncvreg models ‚Äî msaenet.tune.ncvreg","text":"Automatic (parallel) parameter tuning ncvreg models","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.ncvreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatic (parallel) parameter tuning for ncvreg models ‚Äî msaenet.tune.ncvreg","text":"","code":"msaenet.tune.ncvreg(   x,   y,   family,   penalty,   gammas,   alphas,   tune,   nfolds,   ebic.gamma,   eps,   max.iter,   seed,   parallel,   ... )"},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.ncvreg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatic (parallel) parameter tuning for ncvreg models ‚Äî msaenet.tune.ncvreg","text":"Optimal model object, parameter set, criterion value","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.ncvreg.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Automatic (parallel) parameter tuning for ncvreg models ‚Äî msaenet.tune.ncvreg","text":"Chen, Jiahua, Zehua Chen. (2008). Extended Bayesian information criteria model selection large model spaces. Biometrika 95(3), 759--771.","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.ncvreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Automatic (parallel) parameter tuning for ncvreg models ‚Äî msaenet.tune.ncvreg","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.glmnet","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.glmnet","text":"Select number adaptive estimation steps","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.glmnet","text":"","code":"msaenet.tune.nsteps.glmnet(model.list, tune.nsteps, ebic.gamma.nsteps)"},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.glmnet","text":"optimal step number","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.glmnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.glmnet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.ncvreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.ncvreg","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.ncvreg","text":"Select number adaptive estimation steps","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.ncvreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.ncvreg","text":"","code":"msaenet.tune.nsteps.ncvreg(model.list, tune.nsteps, ebic.gamma.nsteps)"},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.ncvreg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.ncvreg","text":"optimal step number","code":""},{"path":"https://nanx.me/msaenet/reference/msaenet.tune.nsteps.ncvreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Select the number of adaptive estimation steps ‚Äî msaenet.tune.nsteps.ncvreg","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msamnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","title":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","text":"Multi-Step Adaptive MCP-Net","code":""},{"path":"https://nanx.me/msaenet/reference/msamnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","text":"","code":"msamnet(   x,   y,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"cox\"),   init = c(\"mnet\", \"ridge\"),   gammas = 3,   alphas = seq(0.05, 0.95, 0.05),   tune = c(\"cv\", \"ebic\", \"bic\", \"aic\"),   nfolds = 5L,   ebic.gamma = 1,   nsteps = 2L,   tune.nsteps = c(\"max\", \"ebic\", \"bic\", \"aic\"),   ebic.gamma.nsteps = 1,   scale = 1,   eps = 1e-04,   max.iter = 10000L,   penalty.factor.init = rep(1, ncol(x)),   seed = 1001,   parallel = FALSE,   verbose = FALSE )"},{"path":"https://nanx.me/msaenet/reference/msamnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","text":"x Data matrix. y Response vector family \"gaussian\", \"binomial\", \"poisson\". family \"cox\", response matrix created Surv. family Model family, can \"gaussian\", \"binomial\", \"poisson\", \"cox\". init Type penalty used initial estimation step. Can \"mnet\" \"ridge\". gammas Vector candidate gammas (concavity parameter) use MCP-Net. Default 3. alphas Vector candidate alphas use MCP-Net. tune Parameter tuning method estimation step. Possible options \"cv\", \"ebic\", \"bic\", \"aic\". Default \"cv\". nfolds Fold numbers cross-validation tune = \"cv\". ebic.gamma Parameter Extended BIC penalizing size model space tune = \"ebic\", default 1. details, see Chen Chen (2008). nsteps Maximum number adaptive estimation steps. least 2, assuming adaptive MCP-net one adaptive estimation step. tune.nsteps Optimal step number selection method (aggregate optimal model step compare). Options include \"max\" (select final-step model directly), compare models using \"ebic\", \"bic\", \"aic\". Default \"max\". ebic.gamma.nsteps Parameter Extended BIC penalizing size model space tune.nsteps = \"ebic\", default 1. scale Scaling factor adaptive weights: weights = coefficients^(-scale). eps Convergence threshhold use MCP-net. max.iter Maximum number iterations use MCP-net. penalty.factor.init multiplicative factor penalty applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. make certain variables likely selected, assign smaller value. Default rep(1, ncol(x)). seed Random seed cross-validation fold division. parallel Logical. Enable parallel parameter tuning , default FALSE. enable parallel tuning, load doParallel package run registerDoParallel() number CPU cores calling function. verbose print estimation progress?","code":""},{"path":"https://nanx.me/msaenet/reference/msamnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","text":"List model coefficients, ncvreg model object, optimal parameter set.","code":""},{"path":"https://nanx.me/msaenet/reference/msamnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msamnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-Step Adaptive MCP-Net ‚Äî msamnet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msamnet.fit <- msamnet(   dat$x.tr, dat$y.tr,   alphas = seq(0.3, 0.9, 0.3),   nsteps = 3L, seed = 1003 ) #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values.  print(msamnet.fit) #> Call: msamnet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.3, 0.9, 0.3),  #>     nsteps = 3L, seed = 1003)  #>   Df    Lambda Gamma Alpha #> 1  3 0.4095024     3   0.9 msaenet.nzv(msamnet.fit) #> [1]  2  4 35 msaenet.fp(msamnet.fit, 1:5) #> [1] 1 msaenet.tp(msamnet.fit, 1:5) #> [1] 2 msamnet.pred <- predict(msamnet.fit, dat$x.te) msaenet.rmse(dat$y.te, msamnet.pred) #> [1] 2.909146 plot(msamnet.fit)"},{"path":"https://nanx.me/msaenet/reference/msasnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","title":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","text":"Multi-Step Adaptive SCAD-Net","code":""},{"path":"https://nanx.me/msaenet/reference/msasnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","text":"","code":"msasnet(   x,   y,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"cox\"),   init = c(\"snet\", \"ridge\"),   gammas = 3.7,   alphas = seq(0.05, 0.95, 0.05),   tune = c(\"cv\", \"ebic\", \"bic\", \"aic\"),   nfolds = 5L,   ebic.gamma = 1,   nsteps = 2L,   tune.nsteps = c(\"max\", \"ebic\", \"bic\", \"aic\"),   ebic.gamma.nsteps = 1,   scale = 1,   eps = 1e-04,   max.iter = 10000L,   penalty.factor.init = rep(1, ncol(x)),   seed = 1001,   parallel = FALSE,   verbose = FALSE )"},{"path":"https://nanx.me/msaenet/reference/msasnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","text":"x Data matrix. y Response vector family \"gaussian\", \"binomial\", \"poisson\". family \"cox\", response matrix created Surv. family Model family, can \"gaussian\", \"binomial\", \"poisson\", \"cox\". init Type penalty used initial estimation step. Can \"snet\" \"ridge\". gammas Vector candidate gammas (concavity parameter) use SCAD-Net. Default 3.7. alphas Vector candidate alphas use SCAD-Net. tune Parameter tuning method estimation step. Possible options \"cv\", \"ebic\", \"bic\", \"aic\". Default \"cv\". nfolds Fold numbers cross-validation tune = \"cv\". ebic.gamma Parameter Extended BIC penalizing size model space tune = \"ebic\", default 1. details, see Chen Chen (2008). nsteps Maximum number adaptive estimation steps. least 2, assuming adaptive SCAD-net one adaptive estimation step. tune.nsteps Optimal step number selection method (aggregate optimal model step compare). Options include \"max\" (select final-step model directly), compare models using \"ebic\", \"bic\", \"aic\". Default \"max\". ebic.gamma.nsteps Parameter Extended BIC penalizing size model space tune.nsteps = \"ebic\", default 1. scale Scaling factor adaptive weights: weights = coefficients^(-scale). eps Convergence threshhold use SCAD-net. max.iter Maximum number iterations use SCAD-net. penalty.factor.init multiplicative factor penalty applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. make certain variables likely selected, assign smaller value. Default rep(1, ncol(x)). seed Random seed cross-validation fold division. parallel Logical. Enable parallel parameter tuning , default FALSE. enable parallel tuning, load doParallel package run registerDoParallel() number CPU cores calling function. verbose print estimation progress?","code":""},{"path":"https://nanx.me/msaenet/reference/msasnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","text":"List model coefficients, ncvreg model object, optimal parameter set.","code":""},{"path":"https://nanx.me/msaenet/reference/msasnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/msasnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-Step Adaptive SCAD-Net ‚Äî msasnet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msasnet.fit <- msasnet(   dat$x.tr, dat$y.tr,   alphas = seq(0.3, 0.9, 0.3),   nsteps = 3L, seed = 1003 ) #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values. #> Warning: ncvreg() is intended for pathwise optimization, not for single values of lambda. #>   1. You are strongly encouraged to fit a path and extract the solution at the lambda value of interest, rather than use ncvreg() in this way. #>   2. In particular, if you are using the MCP or SCAD penalties, be aware that you greatly increase your risk of converging to an inferior local maximum if you do not fit an entire path. #>   3. You may wish to look at the ncvfit() function, which is intended for non-path (i.e., single-lambda) optimization and allows the user to supply initial values.  print(msasnet.fit) #> Call: msasnet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.3, 0.9, 0.3),  #>     nsteps = 3L, seed = 1003)  #>   Df    Lambda Gamma Alpha #> 1  2 0.4417076   3.7   0.9 msaenet.nzv(msasnet.fit) #> [1] 2 4 msaenet.fp(msasnet.fit, 1:5) #> [1] 0 msaenet.tp(msasnet.fit, 1:5) #> [1] 2 msasnet.pred <- predict(msasnet.fit, dat$x.te) msaenet.rmse(dat$y.te, msasnet.pred) #> [1] 2.724263 plot(msasnet.fit)"},{"path":"https://nanx.me/msaenet/reference/plot.msaenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot msaenet Model Objects ‚Äî plot.msaenet","title":"Plot msaenet Model Objects ‚Äî plot.msaenet","text":"Plot msaenet model objects.","code":""},{"path":"https://nanx.me/msaenet/reference/plot.msaenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot msaenet Model Objects ‚Äî plot.msaenet","text":"","code":"# S3 method for msaenet plot(   x,   type = c(\"coef\", \"criterion\", \"dotplot\"),   nsteps = NULL,   highlight = TRUE,   col = NULL,   label = FALSE,   label.vars = NULL,   label.pos = 2,   label.offset = 0.3,   label.cex = 0.7,   label.srt = 90,   xlab = NULL,   ylab = NULL,   abs = FALSE,   ... )"},{"path":"https://nanx.me/msaenet/reference/plot.msaenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot msaenet Model Objects ‚Äî plot.msaenet","text":"x object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet. type Plot type, \"coef\" coefficient path plot across estimation steps; \"criterion\" scree plot model evaluation criterion used (CV error, AIC, BIC, EBIC); \"dotplot\" Cleveland dot plot coefficients estimated model optimal step. nsteps Maximum number estimation steps plot. Default plot steps. highlight highlight \"optimal\" step according criterion? Default TRUE. col Color palette use coefficient paths. NULL, default color palette assigned. label label non-zero variables optimal step coefficient plot dot plot? Default FALSE. TRUE label.vars = NULL, index non-zero variables used labels. label.vars Labels use variables label = \"TRUE\". label.pos Position labels. See argument pos text details. label.offset Offset labels. See argument offset text details. label.cex Character expansion factor labels. See argument cex text details. label.srt Label rotation degrees Cleveland dot plot. Default 90. See argument srt par details. xlab Title x axis. NULL, use default title. ylab Title y axis. NULL, use default title. abs plot absolute values coefficients instead raw coefficients Cleveland dot plot? Default FALSE. ... parameters (used).","code":""},{"path":"https://nanx.me/msaenet/reference/plot.msaenet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot msaenet Model Objects ‚Äî plot.msaenet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/plot.msaenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot msaenet Model Objects ‚Äî plot.msaenet","text":"","code":"# \\donttest{ dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 5L, tune.nsteps = \"bic\",   seed = 1002 )  plot(fit) plot(fit, label = TRUE)  plot(fit, label = TRUE, nsteps = 5)  plot(fit, type = \"criterion\")  plot(fit, type = \"criterion\", nsteps = 5)  plot(fit, type = \"dotplot\", label = TRUE)  plot(fit, type = \"dotplot\", label = TRUE, abs = TRUE)  # }"},{"path":"https://nanx.me/msaenet/reference/predict.msaenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","title":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","text":"Make predictions new data msaenet model object.","code":""},{"path":"https://nanx.me/msaenet/reference/predict.msaenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","text":"","code":"# S3 method for msaenet predict(object, newx, ...)"},{"path":"https://nanx.me/msaenet/reference/predict.msaenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","text":"object object class msaenet produced aenet, amnet, asnet, msaenet, msamnet, msasnet. newx New data predict . ... Additional parameters, particularly prediction type predict.glmnet, predict.ncvreg, predict.ncvsurv.","code":""},{"path":"https://nanx.me/msaenet/reference/predict.msaenet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","text":"Numeric matrix predicted values.","code":""},{"path":"https://nanx.me/msaenet/reference/predict.msaenet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/predict.msaenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make Predictions from an msaenet Model ‚Äî predict.msaenet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  msaenet.pred <- predict(msaenet.fit, dat$x.te) msaenet.rmse(dat$y.te, msaenet.pred) #> [1] 2.839212"},{"path":"https://nanx.me/msaenet/reference/print.msaenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Print msaenet Model Information ‚Äî print.msaenet","title":"Print msaenet Model Information ‚Äî print.msaenet","text":"Print msaenet model objects (currently, printing model information final step).","code":""},{"path":"https://nanx.me/msaenet/reference/print.msaenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print msaenet Model Information ‚Äî print.msaenet","text":"","code":"# S3 method for msaenet print(x, ...)"},{"path":"https://nanx.me/msaenet/reference/print.msaenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print msaenet Model Information ‚Äî print.msaenet","text":"x object class msaenet. ... Additional parameters print (used).","code":""},{"path":"https://nanx.me/msaenet/reference/print.msaenet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print msaenet Model Information ‚Äî print.msaenet","text":"Nan Xiao <https://nanx.>","code":""},{"path":"https://nanx.me/msaenet/reference/print.msaenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print msaenet Model Information ‚Äî print.msaenet","text":"","code":"dat <- msaenet.sim.gaussian(   n = 150, p = 500, rho = 0.6,   coef = rep(1, 5), snr = 2, p.train = 0.7,   seed = 1001 )  msaenet.fit <- msaenet(   dat$x.tr, dat$y.tr,   alphas = seq(0.2, 0.8, 0.2),   nsteps = 3L, seed = 1003 )  print(msaenet.fit) #> Call: msaenet(x = dat$x.tr, y = dat$y.tr, alphas = seq(0.2, 0.8, 0.2),  #>     nsteps = 3L, seed = 1003)  #>   Df     %Dev       Lambda #> 1  8 0.797121 1.219202e+15"},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-31-2019-05-17","dir":"Changelog","previous_headings":"","what":"msaenet 3.1 (2019-05-17)","title":"msaenet 3.1 (2019-05-17)","text":"CRAN release: 2019-05-17","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-3-1","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 3.1 (2019-05-17)","text":"Added detailed signal--noise ratio (SNR) definition msaenet.sim.gaussian(). Updated example code vignette make work better recent version glmnet (2.0-16). Updated GitHub repository links due handle change. Updated vignette style.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-30-2018-12-14","dir":"Changelog","previous_headings":"","what":"msaenet 3.0 (2018-12-14)","title":"msaenet 3.0 (2018-12-14)","text":"CRAN release: 2018-12-14","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-3-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 3.0 (2018-12-14)","text":"Added new argument penalty.factor.init support customized penalty factor applied coefficient initial estimation step. useful incorporating prior information variable weights, example, emphasizing specific clinical variables. thank Xin Wang University Michigan feedback [#4].","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-29-2018-05-13","dir":"Changelog","previous_headings":"","what":"msaenet 2.9 (2018-05-13)","title":"msaenet 2.9 (2018-05-13)","text":"CRAN release: 2018-05-14","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-9","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.9 (2018-05-13)","text":"New URL documentation website: https://nanx./msaenet/.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-28-2018-01-05","dir":"Changelog","previous_headings":"","what":"msaenet 2.8 (2018-01-05)","title":"msaenet 2.8 (2018-01-05)","text":"CRAN release: 2018-01-05","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-2-8","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 2.8 (2018-01-05)","text":"Added Cleveland dot plot option type = \"dotplot\" plot.msaenet(). plot offers direct visualization model coefficients optimal step.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-27-2017-09-24","dir":"Changelog","previous_headings":"","what":"msaenet 2.7 (2017-09-24)","title":"msaenet 2.7 (2017-09-24)","text":"CRAN release: 2017-09-24","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"bug-fixes-2-7","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"msaenet 2.7 (2017-09-24)","text":"Fixed missing arguments issue init = \"ridge\".","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-26-2017-04-23","dir":"Changelog","previous_headings":"","what":"msaenet 2.6 (2017-04-23)","title":"msaenet 2.6 (2017-04-23)","text":"CRAN release: 2017-04-24","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-6","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.6 (2017-04-23)","text":"Added two arguments lower.limits upper.limits support coefficient constraints aenet() msaenet() [#1].","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-25-2017-03-24","dir":"Changelog","previous_headings":"","what":"msaenet 2.5 (2017-03-24)","title":"msaenet 2.5 (2017-03-24)","text":"CRAN release: 2017-03-25","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-5","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.5 (2017-03-24)","text":"Better code indentation style. Update gallery images README.md.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-24-2017-02-17","dir":"Changelog","previous_headings":"","what":"msaenet 2.4 (2017-02-17)","title":"msaenet 2.4 (2017-02-17)","text":"CRAN release: 2017-02-18","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-4","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.4 (2017-02-17)","text":"Improved graphical details coefficient path plots, following general graphic style ESL (Elements Statistical Learning) book. options available plot.msaenet() extra flexibility: now possible set important properties label appearance position, offset, font size, axis titles via new arguments label.pos, label.offset, label.cex, xlab, ylab.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-23-2017-02-09","dir":"Changelog","previous_headings":"","what":"msaenet 2.3 (2017-02-09)","title":"msaenet 2.3 (2017-02-09)","text":"CRAN release: 2017-02-10","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-3","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.3 (2017-02-09)","text":"Reduced model saturation cases improved speed initialization step MCP-net SCAD-net based models init = \"ridge\", using ridge estimation implementation glmnet. benefit, now aligned baseline comparison elastic-net based models MCP-net/SCAD-net based models init = \"ridge\". Style improvements code examples: reduced whitespace new formatting scheme.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-22-2017-02-02","dir":"Changelog","previous_headings":"","what":"msaenet 2.2 (2017-02-02)","title":"msaenet 2.2 (2017-02-02)","text":"CRAN release: 2017-02-02","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-2-2","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 2.2 (2017-02-02)","text":"Added BIC, EBIC, AIC addition k-fold cross-validation model selection. Added new arguments tune tune.nsteps controls selecting optimal model step, optimal model among steps (.e.¬†optimal step). Added arguments ebic.gamma ebic.gamma.nsteps control EBIC tuning parameter, ebic specified tune tune.nsteps. Redesigned plot function: now supports two types plots (coefficient path, screeplot optimal step selection criterion), optimal step highlighting, variable labeling, color palette customization. See ?plot.msaenet details.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.2 (2017-02-02)","text":"Renamed previous argument gamma (scaling factor adaptive weights) scale avoid possible confusion. Reset default values candidate concavity parameter gammas 3.7 SCAD-net 3 MCP-net. Unified supported model family model types \"gaussian\", \"binomial\", \"poisson\", \"cox\".","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-21-2017-01-15","dir":"Changelog","previous_headings":"","what":"msaenet 2.1 (2017-01-15)","title":"msaenet 2.1 (2017-01-15)","text":"CRAN release: 2017-01-15","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-2-1","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 2.1 (2017-01-15)","text":"Added functions msaenet.sim.binomial(), msaenet.sim.poisson(), msaenet.sim.cox() generate simulation data logistic, Poisson, Cox regression models. Added function msaenet.fn() computing number false negative selections msaenet models. Added function msaenet.mse() computing mean squared error (MSE).","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-1","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.1 (2017-01-15)","text":"Speed improvements msaenet.sim.gaussian() vectorization generating correlation matrices. Added parameters max.iter epsilon MCP-net SCAD-net related functions finer control convergence criterion. default, max.iter = 10000 epsilon = 1e-4.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-20-2017-01-05","dir":"Changelog","previous_headings":"","what":"msaenet 2.0 (2017-01-05)","title":"msaenet 2.0 (2017-01-05)","text":"CRAN release: 2017-01-05","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 2.0 (2017-01-05)","text":"Added support adaptive MCP-net. See ?amnet details. Added support adaptive SCAD-net. See ?asnet details. Added support multi-step adaptive MCP-net (MSAMNet). See ?msamnet details. Added support multi-step adaptive SCAD-net (MSASNet). See ?msasnet details. Added msaenet.nzv.() displaying indices non-zero variables adaptive estimation steps.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-2-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 2.0 (2017-01-05)","text":"flexible predict.msaenet method allowing users specify prediction type.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-11-2016-12-28","dir":"Changelog","previous_headings":"","what":"msaenet 1.1 (2016-12-28)","title":"msaenet 1.1 (2016-12-28)","text":"CRAN release: 2016-12-29","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-1-1","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 1.1 (2016-12-28)","text":"Added method coef extracting model coefficients. See ?coef.msaenet details.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"improvements-1-1","dir":"Changelog","previous_headings":"","what":"Improvements","title":"msaenet 1.1 (2016-12-28)","text":"New documentation website generated pkgdown, full set function documentation vignettes available. Added Windows continuous integration support using AppVeyor.","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"msaenet-10-2016-09-20","dir":"Changelog","previous_headings":"","what":"msaenet 1.0 (2016-09-20)","title":"msaenet 1.0 (2016-09-20)","text":"CRAN release: 2016-09-20","code":""},{"path":"https://nanx.me/msaenet/news/index.html","id":"new-features-1-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"msaenet 1.0 (2016-09-20)","text":"Initial version msaenet package","code":""}]
